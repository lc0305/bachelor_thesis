% $Log: abstract.tex,v $
% Revision 1.1  93/05/14  14:56:25  starflt
% Initial revision
% 
% Revision 1.1  90/05/04  10:41:01  lwvanels
% Initial revision
% 
%
%% The text of your abstract and nothing else (other than comments) goes here.
%% It will be single-spaced and the rest of the text that is supposed to go on
%% the abstract page will be generated by the abstractpage environment.  This
%% file should be \input (not \include 'd) from cover.tex.
Highly concurrent network-bound applications like web servers and in-memory databases are the foundation of modern large scale distributed systems. While the performance of NICs and network infrastructure increases every year, improvements in the single-core performance of modern server CPUs seem to have plummeted in the past years. 
Highly concurrent network-bound applications need to counteract by scaling horizontally \textbf{and} vertically. One way to scale vertically in these applications is to implement parallel processing for multi-core scaling on modern CPUs. This thesis takes a look at how this can be achieved and the topics discussed range from hardware and operating system related issues to algorithms for dynamic scheduling in user space. \newline
As part of this study benchmark tests were performed with the in-memory databases Redis (with I/O threading), KeyDB (Redis fork with a multithreaded event loop), Mini-Redis (incomplete Redis server that leverages work-stealing scheduling) and Redis Cluster (shared-nothing database cluster), which all ran on a single (physical) node. In the benchmarks the performance metrics tested were throughput and tail latency. The evaluated results demonstrate that the shared-nothing database Redis Cluster delivers the best results for throughput of all the tested applications. Generally, the tests indicate that the less data is shared in-between processes and the less overhead the algorithms for parallel processing introduce, the better the performance in regards to throughput in uniform load scenarios. However, this study also demonstrates that the shared-nothing approach, besides having other disadvantages, might not be the optimal strategy for lowering tail latencies. Sharing data in-between processes, e.g. by leveraging single-queue, multi-server models and dynamic scheduling strategies, which typically add overhead, can deliver better results in regards to tail latencies, especially when workloads are skewed.

