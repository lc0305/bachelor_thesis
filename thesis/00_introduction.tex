\chapter{Introduction}
Over the past five years (January 2016 - January 2021) the average network traffic at the DE-CIX (Deutscher Commercial Internet Exchange) has more than doubled. Common microservice and fan-out architectures are growing more sophisticated and accumulate increasingly more data streams, worsening the effect of high tail latencies. Applications counteract by scaling horizontally and vertically. Average single core performance of CPUs did not increase as much in comparison. Software and hardware patches against side channel attacks like Meltdown and Spectre have contributed to further worsen this gap in the past years. However, the core count of CPUs monotonically increases every year.  \newline
For example, in 2019 AMD launched their second generation “EPYC” Server CPU series that offers up to 64 cores and 128 SMT “threads”. In 2020 Ampere Computing introduced the “Ultra”, an ARM based Server CPU targeted at cloud computing environments, with 80 CPU cores. In both cases the CPU core counts are per CPU socket, which means that in multi socket server systems the total CPU core count might even be a multiple of that.  \newline
Mid to high-end Amazon EC2 instances, such as the M5n and better, offer network bandwidths from 25 Gbps (burst) to 100 Gbps, thus making the network bandwidth disappear as the first bottleneck in the system for single-threaded highly concurrent network-bound applications in lots of scenarios. \newline
One popular example is the open-source in-memory database Redis whose main logic was strictly single-threaded until Redis Version 6, which was released in 2020. This led to forks by cloud providers such as “Alibaba Cloud” and “Amazon Web Services” \cite{alibaba:redis, aws:redis} in 2018 and 2019 respectively and an open-source fork “KeyDB”. These forks incorporate thread-level parallelism into Redis’ main logic. \newline
According to the Redis developers the preferred way for scaling Redis on a single node is the strict shared-nothing approach with a partitioned Redis Cluster. \newline
This thesis is going to demonstrate that one CPU core does not offer enough performance in certain cases to fully utilize the bandwidth provided by modern Network Interface Controllers (NICs), effectively bottlenecking the system. Therefore, highly concurrent network-bound applications need ways to utilize these cores efficiently for scaling vertically on a node. \newline
In this thesis highly concurrent network-bound applications are defined as applications that establish multiple concurrent communication channels for end-to-end bidirectional communication with the goal of making and/or serving requests as performantly as possible. Common performance metrics are throughput and response times. These highly concurrent network-bound applications are typically driven by events in the network such as incoming HTTP requests, remote procedure calls (RPCs) or database queries, which work at the core of modern large scale distributed systems. \newline
In essence, this thesis is supposed to prove that highly concurrent network-bound applications can benefit from modern multi-core CPUs under certain circumstances. It evaluates different architectures and paradigms for concurrency and parallelism in such applications based on research and benchmark experiments. \newline
Highly concurrent network-bound applications are tightly integrated with lower level software stacks and the underlying hardware, so chapter 2 of this study elaborates on the current state of hardware and Linux that is relevant for network applications. The chapter takes a “deep dive” into event notification interfaces, the Linux network stack, NIC hardware, Data Plane Processing, lower level concurrent data structures and building blocks for state of the art synchronization primitives. It serves as the basis for reflecting about how to get the most performance out of the hardware and APIs provided by the operating system from a user space application developer’s perspective. \newline
Chapter 3 examines practical architectures and paradigms for concurrency and parallelism in network applications. It discusses the essence behind the shared-nothing and shared-something approaches and their implications on highly concurrent network-bound applications. Furthermore, the chapter addresses work distribution and scheduling strategies in the implementations of established and upcoming highly concurrent network applications and library abstractions, such as Redis version 6 with I/O threading, KeyDB, NGINX, libuv, Tokio and Glommio. Another section is devoted to Rust Futures and zero-cost async-await because it enables so-called “green threading” - a paradigm that is common in higher level concurrent programming. Rust futures make this paradigm easy to use for highly concurrent “low level” implementations, while not sacrificing any performance. Rust futures are essential for Tokio and Glommio.
SIMD is another important paradigm for parallelism on modern CPUs. So one section takes a quick glance at parts of network applications that can possibly benefit from SIMD. \newline
A common belief in the developer communities is that in-memory databases are prone to be completely limited by the network bandwidth, which implies that there is no need for parallelism in the main logic. Chapter 4 and 5 about performance evaluations test whether this belief is true by using quantitative benchmark evaluations. These benchmarks test how some of the in-memory database solutions, which are discussed in detail in the chapter about architectures \& paradigms, perform in regards to throughput and response time percentiles under high load in a variety of scenarios. The benchmark evaluations serve as the basis for verifying or refuting some of the research which was conducted on different approaches to concurrency \& parallelism in highly concurrent network-bound applications. \newline
Finally, the conclusion summarizes key findings with regard to the research question that I came across while researching for this thesis by discussing core principles when developing highly concurrent network-bound applications. \newline
Due to the practice-oriented and recent nature of the research question, it is to be expected that this thesis also references lots of grey literature and blog posts among other more reliable sources.