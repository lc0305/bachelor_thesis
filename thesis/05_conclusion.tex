\chapter{Conclusions}
\section{Overview}
A defining quote for this thesis could be: \textit{“The free lunch is over.” } \linebreak
Making this rather humorous statement, in 2005, 16 years ago, Herb Sutter wisley predicted that applications would increasingly need to incorporate parallelism in order to substantially benefit from modern hardware \cite{sutter:free_lunch}. \newline 
This thesis demonstrates that in certain scenarios the question is not \textit{whether} to incorporate parallelism into highly concurrent network-bound applications for scaling on multi-core CPUs, but rather \textit{how} to efficiently benefit from parallelism in these applications.
This study examines \textit{how} different highly concurrent network-bound applications and library abstractions implement concurrency \& parallelism. It captures and classifies conceptual differences between the architectures and paradigms, which these applications make use of, without losing sight of the big picture, while also preserving relevant nuances.
All things considered, the conducted research and the assumptions, which were formulated based on inductive reasoning and logic, are supported by the quantitative performance evaluations. These evaluations demonstrate particularly well that “there is no free lunch” when implementing such systems. There is a trade-off with almost every detail, so decisions about implementation details should be made according to the requirements of the application. \newline
As a conclusion I would like to summarize some of the key findings of the research conducted for this thesis and shed light on possible further developments in this field.

\section{Principles when developing Highly Concurrent Network-Bound Applications}
Developing usually involves dealing with requirements which correspond to a problem that an application is supposed to solve. It is important to define clear requirements and prioritize them. Since “there is no free lunch”, these requirements should be the major influence on architectural decisions. In highly concurrent network-bound applications, such requirements could include concrete numbers in throughput, response time percentiles and further scalability in a distributed environment. In a serverless architecture requirements could even include something like application startup time.  \newline
Knowledge about the data that is served and how it is accessed is crucial. It is important to know how computational effort and payload sizes are distributed among requests.
If most of the payloads are small, bypassing the kernel altogether with a user space network stack should be considered. Large payloads often are files and optimizations such as the sendfile system call can be applied to reduce copies. The network bandwidth or the speed of secondary storage often bottleneck these larger payloads.
Demanding computations or other “blocking” operations within requests should not interfere with making progress in an event driven architecture and can be offloaded to a threadpool,  for example. “Head-of-line blocking” caused by demanding requests in skewed workloads can be reduced by techniques such as Tokios’s task budget. This is supported by Tokio delivering the best P99.9 and P99.99 response time percentiles in the benchmarks for this study.  \newline
Sometimes useful high level information about the data access is available. This information should even be considered for implementing “lower level” details. For example, in large distributed fan-out architectures some requests might have particularly strict response time requirements. This is a scenario where optimizations utilizing techniques similar to the “latency ring” in Glommio could become interesting.  \newline
Essentially, the major differences in respect to parallelism in network-driven architectures are the queueing models and related work distribution and scheduling algorithms. 
One practical example for a trade-off regarding the queueing model is whether to use separate socket instances with “SO\_REUSEPORT” or not. It should also be assessed, whether or not to share the stateful I/O selector instance across multiple threads of execution. Different approaches have different implications on performance, which usually depend on the concrete workload. \newline
Similarly, different strategies of distributing work among multiple threads can result in better or worse performance depending on the workload. Rather “fair” scheduling algorithms, such as the discussed “work-stealing”, tend to share more resources and have more algorithmical overhead. However, they can improve response times and throughput among clients under certain non-uniform load scenarios.\newline
Standard benchmarks, such as the “memtier” benchmark that is used in this study, often test with uniform load distribution among clients, where each simulated client issues as many requests as the server (or the client) can handle, which is the best case scenario for most applications. When data is partitioned, benchmarks usually also generate uniform load on nodes, which means that they do not test highly skewed workloads.\newline
But probably the most important consideration is the question whether to leverage the shared-something or the shared-nothing approach. Sharing resources usually limits the theoretical speedup, especially if process synchronization is involved that decreases \textit{p} in Amdahl’s law. The Redis’ I/O threading that was put to the test in the benchmarks is a good example for an application that scales rather poorly across multiple CPU cores, which is most likely due to a “low” \textit{p} and further overhead associated with its implementation. On the other hand, a “high” \textit{p} can lead to almost linear scaling across multiple cores. This is why shared-nothing should be considered on a single CPU with multiple cores, too. In this study the shared-nothing database Redis Cluster was the only application that achieved almost linear scaling in regards to throughput in some cases. However, shared-nothing approaches and the associated data partitioning come at the cost of either making “non-delightful” transactions substantially more complex or not supporting them at all. Shared-nothing systems are also prone to experiencing hot spot problems. The shared-nothing database Redis Cluster also delivered substantially worse tail latency results than all the other shared-something approaches in this study.\newline
Other important aspects are implementing the finest possible lock granularity or lock-free data structures that remove the need for locking altogether. When locking is needed, different locking algorithms should be considered. The best way is to test and verify multiple approaches by applying the relevant metrics. Furthermore, the client might be able to reduce process synchronization on the server by keeping state local. One example of this is the cluster state on Redis Cluster, which is (also) stored on the client.\newline
Thread-level parallelism is not the only paradigm that enables parallelism on modern CPUs. SIMD instructions can massively improve certain operations on all array-like data structures.  A use case where server applications can substantially benefit from SIMD is the parsing of data exchange formats. \newline
Highly concurrent network-bound applications should minimize the amount of system calls, which are performed for serving requests. On GNU/Linux systems, “io\_uring” seems to redefine the I/O related interfaces of the operating system and offers lots of potential performance improvements including reducing the amount of total system calls. 
Everything in-between the application code and the hardware can bottleneck. As already mentioned, the kernel probably bottlenecks the overall throughput if the network payloads are “small”. However, some performance aspects of the kernel can be improved by simply reconfiguring it. For example, certain threads can be dedicated to receiving interrupts by setting up the IRQ affinity. 
Furthermore, software layers that are added in-between the application code and the hardware could also be potential bottlenecks. The Node.js runtime, for example, is prone to bottlenecking the throughput. \newline
Performance benchmarks should be built into the CI pipeline of these applications and should specifically put the defined requirements to the test. Test cases should be as close to real use cases as possible. Numbers can fool, so it is important to know the error of the results and to test a variety of different software/hardware configurations.

\section{Possible further developments in this field}
One obvious trend for highly concurrent network-bound applications is making use of the shared-nothing paradigm on a single node, which is what the “thread-per-core” model embraces. In the past, many developers considered shared-nothing to be an architectural paradigm inherently linked to distributed computing. However, the growing core counts of state of the art CPUs increase the need for applications that scale (almost) linearly and thus benefit from (all) the cores that the CPU provides. \newline
The creation of light weight “async runtimes” to support the development of high-performance server applications such as Tokio, Glommio or Seastar is another emerging trend. Glommio and Seastar even embrace the shared-nothing paradigm.\newline
“io\_uring” will most likely revolutionize I/O on Linux. However, this adoption might take some time because “io\_uring” ’s “truly asynchronous” interface differs vastly from what Linux and other UNIX-like operating systems have provided in the past. Highly concurrent network-bound applications usually implement the code that handles the I/O as “zero-cost abstractions” on top of Linux APIs and other supported platforms are often “second class citizens”, which is not a substantial problem if the APIs are similar. In order not to miss out on too much performance, other UNIX-like operating systems might be forced to adopt “io\_uring”, too. Glommio, for example, is Linux exclusive and already built on “io\_uring”. Tokio plans to adopt “io\_uring” this year\footnote{2021}. \newline
The boundaries between what is typically done in user space and what is done in kernel space have started to blur: User space network stacks shift functionality that was previously handled by the kernel to user space to remove possible overhead within the kernel. eBPF programs, on the other hand, enable running custom application logic within the kernel. In the coming years, these boundaries will probably be blurred even further. \newline
In regards to hardware, NIC’s will certainly become even faster. Currently, new ARM CPUs, RISC-V CPUs and further hardware acceleration are turning the previously Intel dominated server industry upside down. For example, today AWS offers competitive EC2 instances with custom ARM CPUs (AWS Graviton) and the AWS Nitro System, which is a family of ASICs that accelerate cloud computing environments. I assume that the future might bring server hardware that has an even tighter integration with the NIC and other custom ASICs for hardware acceleration. FPGAs are already popular in software defined networking. Considering the large scale at which cloud providers act, highly concurrent system applications, such as in-memory databases or high performance web servers, might be impacted by the trend of hardware acceleration, too.
