Netzwerkgebundene Anwendungen wie Web Server und in-memory Datenbanken bilden das Fundament für moderne stark verteilte Systeme. Die Leistung von Netzwerkkarten und der dazugehörigen Infrastruktur steigt jährlich, indessen nimmt die Leistungszunahme von \linebreak modernen Prozessoren pro Kern weiter ab. Um dieser Entwicklung entgegenzuwirken müssen diese Anwendungen horizontal und vertikal skalierbar sein. Parallele Algorithmen ermöglichen vertikale Skalierung auf modernen Mehrkern-Prozessoren. Diese Thesis betrachtet wie \linebreak netzwerkgebundene Anwendungen auf modernen Mehrkern-Prozessoren skalieren können. Die dabei behandelten Themen reichen von Hardware und Betriebssystemen bis hin zu \linebreak Algorithmen für dynamisches “Scheduling” im “User Space”.\newline
Als ein wesentlicher Teil dieser Thesis wurden Benchmark-Tests für die in-memory Datenbanken Redis (mit  “I/O Threading”), KeyDB (“Redis fork” mit einer “multithreaded Event Loop”), Mini-Redis (unvollständiger Redis Server, der von “Work-Stealing Scheduling” \linebreak Gebrauch macht) und Redis Cluster (“shared-nothing” Datenbank-Cluster) durchgeführt. Alle getesteten Datenbanken liefen auf einem (physischen) Server. Die Performance-Metriken waren der absolute Durchsatz und “Tail Latency”. In diesen Benchmarks erzielt die “shared-nothing” Datenbank Redis Cluster den höchsten Durchsatz aller getesteten Anwendungen. Im Allgemeinen weisen die Tests darauf hin, dass je weniger Daten unter Prozessen geteilt werden und je weniger Mehraufwand die Parallelverarbeitung verursacht, desto höher ist der Durchsatz den diese Anwendungen bei homogener Last maximal erzielen können. Auf der anderen Seite zeigt diese Arbeit aber auch auf, dass das “shared-nothing” Paradigma, abgesehen von anderer Nachteilen, womöglich nicht die optimalste Strategie für die Senkung von “Tail Latencies” ist. 
Das Teilen von Daten zwischen Prozessen, zum Beispiel durch die Verwendung von “Single-Queue, Multi-Server” Queueing-Modellen und dynamischen “Scheduling” Strategien, kann bezüglich der “Tail Latencies” trotz Mehraufwand bessere Resultate erzielen, vor allem bei “skewed Workloads”.