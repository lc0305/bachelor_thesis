\chapter{Current State of Linux and Hardware for Highly Concurrent Network-Bound Applications}
In order to get the best performance out of modern CPUs in highly concurrent network-bound applications, it is important to take a look at how related APIs are handled on modern operating systems and where they historically came from. Since it is free, open-source and the predominant force in modern cloud infrastructure, this section focuses on the operating system GNU/Linux. However, some information given in this context also applies to other UNIX-like operating systems, but taking all of them into account is beyond the scope of this study. \newline
The first and the biggest section in this chapter captures the current state of networking related APIs, such as “epoll”, “io\_uring” and “UIO” in Linux, and discusses their implications on performance. \newline
The second section picks up where the first subsection left off and serves as a quick introduction to paradigms for concurrency \& parallelism within the network stack (data plane processing). \newline
Security patches against side-channel attacks, such as Meltdown and Spectre, have a significant impact on applications that make frequent use of system calls. This is why the third section discusses the impact of these side-channel attacks on highly concurrent network-bound applications. \newline
Concurrent queues are essential to the implementation of most highly concurrent network-bound applications. So the fourth section discusses the “low level” implementation of a lock-free single-producer, single-consumer and a lock-free multiple-producer, multiple-consumer queue. Lock-free queues rely on atomic operations and so-called “memory barriers” to function correctly. As a consequence, the section also explains the implementation of these atomic operations and memory barriers on state of the art instruction set architectures. \newline
Lock-free implementations are not viable in every scenario. When locking is required, it should be implemented as efficiently as possible. The last section in this chapter explains efficient “user space locking” with the Linux system call “futex”.
\section{Concurrent Connections - From select to io\_uring and DPDK}
\subsection{select, non-blocking and the Reactor Pattern}
Linux like almost all modern operating systems ships with APIs for TCP and UDP networking, so-called socket APIs. This provides an abstraction over the concrete network stack and the hardware. The idea is that applications should not have to implement a TCP/IP stack or know how to communicate with every available networking controller. Networking in kernel space also provides memory protection and the kernel is able to handle resource allocation for multiple processes that need concurrent access to the network. \newline
Within the Linux Kernel resides the networking subsystem. The kernel provides a set of well-defined system calls to enable networking functionality within a user space application.
UNIX-like and POSIX-compliant operating systems typically implement the BSD sockets API for networking. As seen from the perspective of a server the basic operating principle of this API is to bind a socket, that is the combination of an IP address and a transport layer port, to a local socket address, then listen on that socket and accept incoming connections. However, this does not make multiple concurrent connections possible. \newline
To achieve multiple concurrent connections, a naive approach is to assign a thread or a process to each new accepted connection, but implementing massive networking concurrency using thread-level parallelism is considered to be a pitfall with today’s systems \cite[1245]{kerrisk:linuxapi}\cite{kegel:c10k}. The overhead of context switching between the threads results in bad performance once a certain amount of concurrent connections is reached. This is explained in greater detail in the chapter 3 about architectures and paradigms. \newline
In this “thread-per-connection” approach the sockets are in blocking mode. This is also the default behavior for networking sockets. Any operation on a blocking socket can theoretically “block” at any time. “Blocking” in this context means that in the case of reading, for example, the kernel buffer is not ready to be read from because no data has arrived at the socket yet. In the case of writing it means that the kernel buffer is not ready to be written to. The kernel effectively blocks the thread until the intended operation is ready. On highly concurrent systems this yields bad performance and might even impact the liveness of the system. If on the other hand this socket is in non-blocking mode, the intended operation returns immediately with an error and the value of “errno” is set to “EAGAIN” or “EWOULDBLOCK”. The “O\_NONBLOCK” mask that is used to set descriptors which identify sockets in non-blocking mode, has no effect on descriptors identifying regular files.
Now, another rather naive approach would be to put these file descriptors in non-blocking mode and poll each of them on a single thread until it is ready to perform the intended I/O operation. This solves the problem of the context switch overhead, but would just waste CPU cycles in many cases.\footnote{While manually polling file descriptors might not be efficient, polling for data in other cases, such as within drivers for highly performant I/O devices, can be more efficient than asynchronous interrupt based approaches as it is be explained later in the section on network drivers.} \newline
To mitigate the need for applications polling manually on descriptors, modern operating systems usually ship with multiple I/O notification strategies. An example for an I/O notification selector is the select system call in UNIX-like and POSIX-compliant operating systems, whose history goes back to the 1980s. Nowadays, the select system call is considered to be deprecated for multiple reasons that are not the main subject of this thesis, but the basic interface has remained very similar in more modern event notification interfaces like "epoll" or "kqueue":\newline
In “selectish” event notification interfaces the application registers its interest in particular events that it wants to receive notifications of with the associated file descriptors. For example, in the case of “select” an application can register a read interest with a file descriptor identifying a socket by adding it to the specific set for this event. There is a set of file descriptors for read, write and error events. The application needs to pass these sets of file descriptors as an argument to the select system call. The select system call either returns if at least one of the file descriptors becomes “ready”, which means that the operation associated with the interest can be performed without blocking, or if the specified timeout is exceeded. In order to notify the application of their readiness the sets of file descriptors are mutated in-place by the kernel. This is one of the many pitfalls of the classic select system call. They then need to be checked by the caller in user space for events.
A “reactor style” application handles these events by calling a callback function that was registered earlier and is owned by each handle (file descriptor), a so-called Event Handler. The entity in the program which calls “select” (Synchronous Event Demultiplexer) and handles the events is called Initiation Dispatcher. The application typically repeats this process in a loop and this is called an event loop. One iteration is called an event loop tick \cite{schmidt:reactor}. \newline
The select system call has many problems. One of them, the in-place mutation of the “fd\_set”s, was already mentioned earlier. It forces the application to always copy the “fd\_set”s before passing them to the “select” call. Moreover, when using the “glibc wrapper” for “select”, an “fd\_set” can only hold a maximum of 1024 file descriptors and no descriptor can have a greater value than 1023 \cite{man:select}. \newline
In comparison to “select” the poll system call improves the ergonomics by fixing issues surrounding “fd\_set”, otherwise its functionality is equivalent \cite{man:poll}.

\subsection{epoll}
“Epoll” is the further evolution of the select and poll system call on Linux.
“Select” and “poll” are stateless, which means that the caller passes the file descriptors of interest with each new call and the kernel then checks each file descriptor for readiness concerning the specified events. This leads to a time complexity of \textit{O(number of descriptors in the passed interest set)} for the select and the poll system call. This is inefficient, especially when a server has lots of concurrent connections and only very few events happen on these connections. \newline
“Epoll” on the other hand is stateful. Before calling “epoll\_wait”, the application sets up an “epoll” instance with “epoll\_create”. This “epoll” instance is a kernel data structure which holds the associated interest and ready list. File descriptors can be added to the interest list by calling the “epoll\_ctl” system call with the event masks that the application wants to be notified of passed as arguments. When an event of interest occurs on one of the monitored descriptors in the interest list, the Linux kernel adds the descriptor to the ready list. When the application calls “epoll\_wait”, the ready list is returned to the caller. This results in “epoll” having a time complexity of \textit{O(number of “ready” descriptors)}. It is the reason why “epoll” is generally more efficient than “select” and “poll”, especially in highly concurrent environments. \newline
Level-triggered “epoll” shares the same semantics with “poll” and is the default.
As an alternative to level-triggered “epoll” notifications, edge-triggered “epoll” notifications\footnote{set with the “EPOLLET” mask} are delivered only when events of interest occur on the monitored descriptor. That means in practice, a call to “epoll\_wait” only notifies the caller(s) about a descriptor being ready once. After that, provided that no other events take place, there are no further notifications, even if there is still data left and ready to be read on that particular descriptor. 
Edge-triggered “epoll” can be more efficient than level-triggered “epoll”. It is recommended  to use edge-triggered “epoll” with non-blocking descriptors and read from and write to a ready descriptor until it returns with “errno” set to “EAGAIN” \cite{man:epoll}. The downside of this recommendation is that big reads or writes could “starve” other registered descriptors.
However, in a multi-threaded context, edge-triggered “epoll” is particularly attractive because it avoids “thundering herd” wake-ups of multiple threads which are blocked by “epoll\_wait”. When an edge-triggered descriptor becomes ready, only one of the blocked threads is “woken up”.

\subsection{io\_submit}
The Linux kernel supports asynchronous I/O notifications, using the “io\_submit” system call family. This API was originally created for asynchronous (unbuffered) file I/O and does not offer truly asynchronous networking I/O, but strangely enough, it does support stateful I/O notifications on sockets similar to “epoll” since Linux kernel version 4.18. “io\_submit” supports adding multiple file descriptors with events of interest to the interest list by making only one system call similar to the kevent system call on BSD and Darwin. This is an advantage over “epoll” that could in theory reduce system call overhead. \newline
“io\_submit” does also allow system call batching for sockets. The application can add multiple read/write requests to a list and then pass this list to “io\_submit”. The read/write requests are executed synchronously, requiring only one context switch to kernel space, which could reduce system call overhead even further  \cite{man:io_submit, majkowski:io_submit}. If the sockets passed to “io\_submit” are non-blocking the process returns to user space after performing all non-blocking read/write operations synchronously.
The team behind Redis tried implementing this API for Redis because Redis’ performance is significantly impacted by system call overhead.\footnote{This is especially true since the Meltdown/Spectre patches.} Salvatore Sanfilippo, the former lead developer, claims to have obtained bad performance by replacing “epoll” with “io\_submit” \cite{antirez:io_submit2}. Unfortunately, a Redis branch using “io\_submit” was not publicly available for testing and to the best of my knowledge, Sanfilippo did not release any concrete numbers for verification.

\subsection{io\_uring and the Proactor Pattern}
“io\_uring” is on its way to become the new standard for asynchronous I/O operations in Linux. It was first introduced in upstream Linux kernel version 5.1 and thus is a relatively young API. Instead of improving the support for established Linux AIO, it introduces a completely new interface. Its goals are ease of use, extensibility, feature richness, efficiency and scalability. “io\_uring” wants to achieve those goals by providing a generic, yet powerful, set of data structures and communication channels with the kernel to the user space application. \newline
When utilizing “io\_uring” the user-space application places I/O requests (submission events) in the “SubmissionQueue” (SQ) and the Linux kernel processes the events and places the “result” in the form of completion events in the “CompletionQueue” (CQ). Then, they are ready to be retrieved by the user-space application. So in essence, “io\_uring” is a relatively lightweight abstraction that implements two producer-consumer queues to establish a bidirectional communication channel between user and kernel space for I/O related operations. 
Unlike previous Linux AIO which only worked with unbuffered regular file I/O this interface works truly asynchronously with all kinds of I/O, most notably with regular buffered file I/O and networking I/O. \newline
The “epoll” instance, which holds the interest and ready list and is essential to the popular “epoll” interface, is only mapped into kernel space. One consequence of this design choice is that the data structure can only be accessed via system calls, which can be comparable expensive in I/O heavy applications, especially since the Meltdown and Spectre patches.
The “SubmissionQueue” and “CompletionQueue” of “io\_uring” on the other hand are mapped into user and kernel space. Both are implemented as single-producer, single-consumer ring buffers and work completely lock-free. The section about concurrent queues goes into more detail about how such queues can be implemented. If kernel side polling for the “SubmissionQueue” is enabled, processing I/O without performing a single system call is possible as long as the application drives the I/O \cite{linux:io_uring}. \newline
The architecture of “io\_uring” makes the Proactor pattern a suitable fit for implementing highly concurrent network-bound applications. 
While the discussed Reactor pattern is designed on the basis of a Synchronous Event Demultiplexer such as “epoll”, which can be queried for notifications on \textbf{operation readiness}, the Proactor Pattern is designed on the basis of an Asynchronous Operation Processor, which accepts operations, \textbf{asynchronously executes} them and later notifies the completion dispatcher of their completion. In the Proactor pattern the completion triggers the dispatch of an associated Completion Handler \cite{schmidt:proactor}. This is similar to how an event triggers the dispatch of the Event Handler in the Reactor pattern. However, this Completion Handler, unlike the Event Handler, does not execute synchronous I/O operations, but instead queues more asynchronous I/O operations. In this pattern “io\_uring” acts as the Asynchronous Operation Processor. With previous I/O selectors, such as “epoll”, an Asynchronous Operation Processor had to be emulated with software in user space, which led to the Reactor pattern being the preferred choice in highly concurrent network-bound applications until now. \newline
Benchmark tests for “io\_uring” are promising and show significant performance gains over the “epoll” API for networking \cite{github:io_uring_bench}. Like kqueue in 2000, “io\_uring” might revolutionize I/O on UNIX-like operating systems, for the first time offering a “truly asynchronous” generic I/O interface, which actually works with all kinds of I/O and which can in theory remove the need to perform system calls for I/O operations completely.
However, it might take some time for major applications to adopt it because the API differs so drastically from the established I/O selectors provided by current UNIX-like operating systems that it is not just a drop-in replacement.

\subsection{Avoiding copies with MSG\_ZEROCOPY and sendfile}
\paragraph{MSG\_ZEROCOPY}\mbox{}\\
As the name suggests MSG\_ZEROCOPY makes zero copy writes to a socket possible and enables truly asynchronous socket I/O on Linux. The kernel notifies the process when it is safe to reuse the previously passed buffer by placing completion notifications on the socket error queue. The queue has to be polled by the application. It is a trade off between per byte copy cost and the page accounting plus the completion notification overhead, which are required for the feature to work as intended. MSG\_ZEROCOPY is supposed to pay off with writes over around 10 KB \cite{linux:zerocopy}.
\paragraph{sendfile}\mbox{}\\
The sendfile system call on Linux enables data transfers between two file descriptors within the kernel space, thus it skips the usual unnecessary copy of the buffer to the user space. Sendfile pays off in particular when large static “regular” files are transferred over a socket, such as it is often the case in web server applications. This approach is also known as “zero-copy transfer” \cite[1260-1262]{kerrisk:linuxapi}\cite{man:sendfile}.

\subsection{Linux Network Stack and User Space Network Drivers}
In order to be able to discuss user space network drivers’ improved performance, it is important to explain how data gets from the socket APIs provided by Linux to a network interface controller (NIC). \newline
The Linux kernel stores a “socket” structure for each connection which is identified by a file descriptor. The “socket” has a “sock” structure which describes the corresponding INET socket. This “sock” structure owns a linked list of “sk\_buff” (socket buffer) structures. 
The socket API pushes the socket buffers to the underlying protocol implementations which add lower layer data, such as port number and network addresses. \newline
In the network subsystem the matching “net\_device” (network device) structure is selected based on IP routing rules. This structure is at the core of the network driver layer and has the methods for acting upon the networking device. For example, for transmitting data the “sk\_buff” is passed to the “hard\_start\_xmit” function, which is implemented by the device driver \cite{coru:device_drivers, linux:networking}. \newline
From here on the implementations depend on the networking device, i.e. a specific NIC. So, as an example, this section describes how the driver for the Intel 8256x NIC device family works. However, this information may apply to a wider variety of NICs. \newline
The NIC is connected via the Peripheral Component Interconnect Express (PCIe). The driver running on the host can communicate with the NIC via configuration registers which are mapped to physical RAM (MMIO). The PCIe Core on the NIC is responsible for bus communication, memory mappings and sending interrupts to the host’s CPU. It has a direct memory access (DMA) Engine, which handles the receive and transmit data transfers as well as descriptor transfers between the host memory and a small amount of on-chip RAM for temporary buffering, which is filled and consumed by the Ethernet Transceiver (PHY). The driver initializes the configuration registers, so that the DMA  Engine “knows” where the RX and TX descriptor queues are located in the host’s physical memory. These descriptor queues are implemented as ring buffers and contain the descriptors with each one “pointing” to a specific packet buffer. The NIC driver consumes RX descriptors and the associated RX packet buffers and produces TX descriptors and associated TX packet buffers based on the input of the kernel. The roles are reversed for the DMA  Engine \cite{intel:8256}. \newline
Moreover, the NIC driver can either work in interrupt or poll mode. The NIC notifies the driver of events by sending interrupts over the PCIe core. In interrupt mode the CPU acts upon these interrupts, e.g. it reads a RX descriptor off the RX descriptor queue, when a new packet arrives. Receiving and acting upon the interrupts is not for free though and creates overhead. When the NIC is busy and new packets arrive continuously, it can be more efficient to mask the interrupts and repeatedly poll the RX descriptor queue instead.
\paragraph{User space network drivers}\mbox{}\\
User space network drivers are bypassing the kernel on Linux by utilizing the Linux UIO API. UIO maps the MMIO and interrupts to pseudo files, which the user space process is granted access to. To work around the problem of the virtual memory in user space, which does not guarantee that underlying physical addresses stay the same, applications make use of “hugepages” because the Linux kernel guarantees that their physical location does not change during runtime to enable safe communication with the NIC \cite{linux:dpdk}. So the RX and TX descriptor queues and their associated packet buffers are entirely mapped into user space with “hugepages”. By using these techniques all the kernel abstractions and features are bypassed effectively and the user space application gets direct access to the NIC. \newline
The Data Plane Development Kit (DPDK) is an open-source project that enables such user space network stack and is currently managed by the Linux Foundation. \newline
There are certain scenarios, where this approach has clear advantages over a kernel network stack: \newline
Context switches to kernel space come at a cost, even more so since the patches against Meltdown/Spectre.\footnote{More on that in the section about the impact of side-channel attacks.}\newline
There is also a considerable amount of “overhead” in the generic socket and networking APIs within the Linux kernel until the data is passed to the driver. The smaller the transmitted payloads are, the more substantial the associated overhead within the kernel is and at a certain point a hard limit is reached within the kernel, which, depending on the payload size, might be below what the NIC is actually capable of.
In this scenario bypassing the kernel altogether in favor of a user space network driver and TCP/IP stack becomes attractive to make the most of the NIC’s capabilities.
Working with user space network drivers below this threshold is not recommended as they also have several disadvantages: \newline
For one, the application’s security deteriorates. Developers should program with extra care due to the process' ability to directly access the NIC. \newline
Apart from the operating system and the application, the user space driver also has to be updated during operation. \newline
Depending on the NIC, the application might also have a monopoly on it. This is not a viable solution if multiple applications in the system are in need of concurrent access. \newline
The application gets direct access to the ethernet frames, meaning that if the application wants to bypass the kernel completely, it needs to implement its own additional (TCP|UDP)/IP stack in user space and perhaps a socket API for convenience, too. OpenFastPath is an example for an open source TCP/IP stack which can run together with DPDK in user space \cite{soft:opf}. It also provides convenience wrappers for the “BSD socket API” and popular I/O notification selectors such as “epoll”. However, using those wrappers might result in worse performance.

\section{Concurrency \& Parallelism in Data Plane Processing}
Data plane processing refers to the computation that is required for transmitting packets/frames from one interface to another in a computer network. User space drivers move data plane processing from kernel to user space. The subsection on the Linux network stacks already described the basic principle of how a NIC driver can communicate with a NIC. It also mentioned some of the advantages and disadvantages of using network drivers in kernel and user space. However, it did not go into detail about strategies to efficiently process received and transmitted packets.
\subsection{Pipeline Model}
In the pipeline model the package processing is divided into stages, with each stage requiring roughly the same computing power. Each stage is assigned to a thread.
However, this model has a few disadvantages. For one, it is nearly impossible to perfectly divide the processing into stages that require the exact same amount of computing power. Depending on how well the process is divided into stages this might even significantly waste resources. On top of that, communicating between the pipelines might require memory accesses which could bottleneck the system. Finally, each packet is not core-local and due to the fact that state of the art CPU cores have “large” non-shared lower level caches nowadays, this might even further worsen the performance \cite{intel:packet}.
\subsection{Run-to-completion Model}
In the run-to-completion model the package processing is not divided into stages. Instead packets are assigned to each core/thread and each core/thread aims to handle the complete processing of those packets from beginning to end. The run-to-completion model addresses problems of the pipeline model. In theory, it wastes less resources and requires no communication between pipeline stages because there are none. This results in less memory accesses and the packets are kept core-local. \newline
When dealing with dedicated hardware accelerators, such as hardware for checksum offloading or field-programmable gate arrays (FPGAs), or the CPU needs to wait on some other resource, the thread can make use of a cooperative multitasking approach. With this approach a task returns to a scheduler whenever it has to wait on a resource \cite{intel:packet}. One problem of the run-to-completion model is possible contention on the NIC resources by the multiple threads. However, modern NICs have multiple descriptor queues which can be mapped in accordance to the number of threads, reducing contention.

\section{Impact of side-channel Attacks}
Research indicates that the Meltdown and Spectre security patches result in a significant slowdown of system calls. Above all, these patches enabled kernel page-table isolation (KTPI) and measurements to avoid indirect branch speculations. KTPI forces processes to maintain separate page tables for kernel and user space to fix Meltdown. Subsequently, this leads to translation lookaside buffer (TLB) misses, which results in a performance decrease by up to 63\% on socket reads \cite{acm:linux_perf}. Spectre was apparently\footnote{“Apparently” because this seems to be architecture-dependent.} patched using a software construct called “retpoline” to prevent branch-target-injection \cite{turner:retpoline}, which adds around 30 CPU cycles to each indirect jump or call and which slows down “epoll” by 72\% \cite{acm:linux_perf}. Other security mechanisms which were patched in the Linux kernel such as “SLAB free list randomization” and “hardened user copy”, whose aim is to reduce the effect of buffer overflow attacks within the kernel, as well as new features and kernel misconfigurations can also result in performance decreases with system calls. Overall, Ren’s et al. paper “An Analysis of Performance Evolution of Linux's Core Operations” (2019) suggests that Redis’, Apache’s and NGINX’s performance can be sped up by 56\%, 33\% and 34\% respectively by mitigating and disabling various performance decreasing patches \cite{acm:linux_perf}. \newline
The gist of the paper is that users should check if they really need all those patches and if the Linux kernel is properly configured to their needs. Disabling patches and reconfiguring the kernel can lead to significant performance increases. \newline
In conclusion, avoiding system calls in network bound applications whenever possible seems to be the right approach. 

\section{Lock-free Concurrent Queues and Memory Barriers}
The data structure that drives efficient multiprocessor workloads is the ring buffer. In computer I/O the ring buffer data structures typically serve as “First In, First Out” (FIFO) queues in the common producer-consumer pattern. The ring buffer stores data in a single fixed-size array and treats it as if the ends were logically connected. The structure also stores a tail and a head “index” for this array. The tail is used by the producer and the head is used by the consumer for indexing the array. When the producer pushes a value onto the queue, it updates the tail and when the consumer pops a value off the queue, it updates the head. When the queue is full, old values can either be overridden or the process waits until a value is popped off the queue. A third possibility is that enqueueing simply returns with an error. When the queue is empty, the process can either wait until a value is pushed onto the queue or return with an error. These queues can be useful for many purposes. For example, the RX/TX descriptor queues that were already mentioned are implemented as ring buffers. The “SubmissionQueue” and the “CompletionQueue” of “io\_uring” are implemented as ring buffers as well. And message queues for message passing between concurrent processes can be implemented as FIFO ring buffers, too. \newline
There are different implementations with different thread-safety guarantees, which can be selected depending on the needs of the application. Single-producer, single-consumer (SPSC) Queues for the communication between two threads (1:1) are very common. One implementation of such a queue is the "WaitFreeQueue<T>" described by Herlihy  \cite[45-48]{herlihy:art_of_mp}. This implementation is very simple, because it requires no locking at all and the possible concurrent access creates little overhead. The pushing onto the queue does not have to be synchronized because only one thread is allowed to produce values; neither does the popping of values off the queue because only one thread is allowed to consume values. But the pushing and popping both require reading head and tail values. Therefore, the load and the store operations performed on the head and tail have to be atomic. \newline
When a program is running on one CPU core, on modern systems each load and store operation is guaranteed to be visible in the correct order (obviously), but depending on the memory model implemented by the CPU possibly \textbf{only} on that core. Modern processors usually do not execute the instructions of a program in strict sequential order for performance reasons and each CPU core typically has its own lower level cache(s). For these reasons, depending on the memory model implemented by the processor, the data produced by a thread can be visible in an arbitrary order to another concurrent thread by default. Compilers for “lower level” languages like C, C++ or Rust are also allowed to reorder the code for performance optimizations. \newline
This could become a problem in Herlihy’s “WaitFreeQueue<T>”: The thread consuming the entries could see an incremented tail index before the data written to the array becomes visible, which might result in undefined behavior.  \newline
Herlihy’s "WaitFreeQueue<T>” is implemented in Java. With the memory model implemented by the JVM (Java Virtual Machine), Herlihy’s "WaitFreeQueue<T>"  works under the condition that the head and tail are atomic or volatile variables \cite[63]{herlihy:art_of_mp}. However, “lower level” languages, like the ones mentioned above, cannot rely on the memory model of a higher level virtual machine. \newline
“Lower level” languages, like C, C++ or Rust, which are typically compiled to native machine code, are agnostic about the memory model of the instruction set architecture (ISA) that the code is compiled for. In order to be able to deal with various memory models on different CPU architectures, their “abstract machines” introduce so-called memory barriers. Loads and stores to atomic variables are typically supported by memory barriers. For other concurrent threads they provide different visibility of load and store operations and ordering guarantees before and after the access to the atomic variable. Compilers map these abstract memory barriers to the concrete memory model of the target architecture and restrict potential reorderings, which they would otherwise be allowed to perform on the instructions.  \newline
Depending on the CPU hardware, this mapping typically makes use of special load and store instructions. On Intel/x86-64 all load and store instructions by default offer the relatively strong “acquire-release” semantic, which is sufficient for the correctness of many algorithms. This is discussed in detail when the “lower level” implementation of Herlihy’s "WaitFreeQueue<T>" is addressed. “Acquire-release” does not give any guarantees regarding global ordering of visibility in-between “independent” atomic variables in different concurrent threads. To achieve this global ordering, sequential consistency is required \cite{cppref:memory_order}. On Intel x86-64 sequential consistency can be achieved using the “MFENCE” memory-fencing instruction, the “LOCK” prefix or special instructions with implicit “LOCK” prefix, like the exchange (XCHG) instruction \cite{intel:memory_order}. Compare-and-swap (CAS) operations with “CMPXCHG” require such a “LOCK” prefix and are thus sequentially consistent by default. There is also no difference between weak and strong CAS on x86-64. According to the C++ specification, weak CAS is allowed to fail spuriously. This means it could fail although the result of the comparison is true. CAS is explained in more detail in a few paragraphs.  \newline
“ARMv8”, on the other hand, implements a weak memory model. Standard load and store operations with “LDR” and “STR” provide relaxed memory order. Sequential consistency, and thus “acquire-release” semantics, too, can be achieved by using the load-acquire register (LDAR) and the store-release register (STLR) instruction. Since ARM requires explicit load and store operations, there are no dedicated atomic read–modify-write operations. ARM has special “exclusive” load and store instructions for implementing such atomic operations. For exclusively loading  “ARMv8” provides the load exclusive register (LDXR) and load-acquire exclusive register (LDAXR) instruction and for storing there is the store exclusive register (STXR) and store-release exclusive register (STLXR) instruction \cite{arm:v8}. Interestingly, store operations on “ARMv8” can fail in case of contention on the cache line related to the exclusively loaded address and the status result is written to a third register. In case such a failure is not acceptable, the CPU has to loop until the read-modify-write operation is executed successfully. This is processor spinning exposed to the developers and sort of optimistic concurrency control within the CPU.\footnote{As a result, there \textit{is} a difference between weak and strong CAS operations on ARM and this is the reason why it is recommended to use weak CAS inside and strong CAS outside of loops.}\newline
Modern CPUs try to do as little stalling and make as few memory accesses as possible. They go to great lengths to only lock the bus when it is necessary.
So they try to enable the memory guarantees requested by the application within the CPU caches by using complex cache-coherence protocols \cite[473-476]{herlihy:art_of_mp}. Still, these instructions and sharing memory between CPU cores due to coherence traffic in general often come at a significant cost. It is important to use the lowest barriers that enable the algorithm to work correctly. \newline
To make Herlihy’s "WaitFreeQueue<T>" work on “lower level” languages, memory barriers are necessary. This rather simple SPSC queue only requires two barriers to function correctly. The store operation of the tail value has to be implemented with a release barrier after a value was pushed onto the queue. This guarantees that no reading or writing operations in the current thread can be reordered \textbf{after} this store. The load operation of the tail value in the consumer thread has to be implemented with an acquire barrier. The acquire barrier guarantees that no reading or writing operation in the current thread can be reordered \textbf{before} the load. The combination of both barriers is also called “acquire-release” protocol. It guarantees that before the atomic store, all write operations in the one thread are visible to other threads that acquire the same atomic variable. So these barriers provide sufficient guarantees for Herlihy’s "WaitFreeQueue<T>" to work correctly. All the other atomic load and store operations can be done relaxed, which means that ordering guarantees are not required before and after the access of the atomic variable. The code for a lock-free SPSC ring-buffer FIFO queue, which I have implemented in modern C++ and which is very similar to Herlihy’s "WaitFreeQueue<T>" can be found in the appendix. The implementation of “io\_uring” described in an earlier section makes use of a similar queue to implement the “SubmissionQueue” and the “CompletionQueue”, which are used for communicating I/O requests and completions between kernel and user space.
\paragraph{Multiple-Producer, Multiple-Consumer Queue}\mbox{}\\
However, a SPSC queue is not always sufficient. Algorithms with higher concurrency often require M:N communication channels and thus a multiple-producer, multiple-consumer (MPMC) queue or something in between like a multiple-producer, single-consumer (MPSC) queue. The trivial solution is to utilize the given SPSC queue and put a lock around the enqueuing, the dequeuing or both. This can be the best solution depending on the queue’s workload \cite{lerche:scheduler}. \newline
However, it is also possible to build entirely lock-free MPMC queues. All these implementations rely on compare-and-swap (CAS) operations: \newline
Compare-and-swap operations are possible with most modern CPUs. They enable comparison between data stored in a memory location and another value, which was often read from that memory location previously. If both values are the same, the CAS operation stores a third new value. All of this is done in a single atomic operation with the caveat that the data CAS can operate on is limited in size. That is to say, on modern x86-64 CPUs the limit for atomic read-modify-write operations is set to 16 Bytes. Considering that the word size and thus the pointer size on x86-64 CPUs is already 64 bit (8 Bytes), this is fairly limited. It also implies that the array in a generic circular lock-free MPMC queue is limited to storing pointers so that multiple parallel accesses on the consumer and the producer side are possible. Consequently, the array has to be atomic. \newline
The following paragraph explains the basic idea behind one implementation of such a MPMC lock-free queue. The queue is heavily inspired by the talk “An Interesting Lock-free Queue” that Tony Van Eerd gave at the CppCon in 2017 \cite{eerd:queue}. \newline
The main array of the queue stores a pointer together with an integer representing the generation of the pointer. In my implementation the generation is represented by an unsigned 64 bit integer, making the total memory footprint of the structure 16 Bytes. This means it is just within the boundaries of CAS operations on x86-64. 
The pointer in this structure can be null or it can store a valid memory address which points to another structure. Using CAS operations, multiple concurrent threads can exchange this generational pointer within the array. When one producer wants to push another entry into the queue, it simply iterates to the first generational pointer within the current generation that is null. Then, the thread initializes the new generational pointer and tries to store it in the specific location using a CAS operation, which succeeds if the previously stored old generational pointer did not change. If the operation fails, the thread repeats the whole process. The dequeuing works analogously, except that the thread iterates to the first generational pointer that stores a valid address and tries to replace it with a new incremented generational pointer, which points to null. \newline
The generation is logically incremented with each new cycle through the circular array, so that threads always “knows” when a pointer has been updated, even if this pointer still points to the same address. So pairing of the generation with the pointer fixes the “ABA Problem” \cite[223-238]{herlihy:art_of_mp}.\newline
To speed up the iteration, which would otherwise be a costly search with a complexity of \textit{O(n)}, the queue also stores atomic head and tail values for indexing similar to the SPSC queue. However, these values do not represent the exact position of the current head and tail pointer. They are rather an approximation. The true head and tail values are guaranteed to be more recent (larger) or equal to the stored head and tail values, resulting in eventual consistency. To determine this “happened-before-relation”, the head and tail indexes need a generational stamp as well. With each queue/dequeue operation the executing thread tries to exchange the current tail/head for its own updated version of tail/head. It is important that all previous stores done by the thread become visible to all other producer and consumer threads before updating the tail/head values with a CAS operation. This optimization makes the best and common case queue/dequeue an \textit{O(1)} operation, just like the SPSC queue, and in the worst and unlikely case an \textit{O(n)} operation.\newline
Disadvantages of this queue compared to the SPSC queue are that it only stores generic pointers to structures, while the SPSC can store structures of arbitrary size directly within the array, resulting in a memory indirection and worse cache locality for the MPMC queue. The pointers to the allocated structures in the array also require further memory management in order not to be invalidated, while at least one thread is still accessing the memory the pointer points to. This could be done by a reference count, for example. It is important not to make a mistake, otherwise this could lead to undefined behavior. This queue requires multiple CAS operations, while the SPSC queue requires no CAS operations to work correctly. CAS operations can be comparatively expensive depending on the underlying CPU architecture. Overall, this MPMC queue has more overhead compared to the SPSC queue.\newline
My implementation of this queue in modern C++ can be found in the appendix of this thesis.
Some of the key benefits that ring buffer data structures provide for I/O heavy workloads including highly concurrent network-bound applications are the following: 
They allow for \textit{O(1)} access and can be implemented lock-free as demonstrated above. Furthermore, they can serve as FIFO queues for the common producer-consumer pattern. And what is important for modern CPUs: They are cache friendly because the data is stored in one contiguous chunk of memory. This is in accordance with what Bjarne Stroustrup says about data structures: Do not store data unnecessarily; keep data compact and access memory in a predictable manner \cite{stroustrup:lists}. 

\section{Fast User Space Locking with futex}
The previous section already implied that concurrent lock-free data structures might not always be the best fit for every problem. These data structures can become very complicated and the associated overhead that makes the data structure lock-free in the first place can become more expensive than using locks with non- or partially-concurrent data structures. When there is low contention on the lock, the inefficient part about “conventional” non-spinning mutex locks is the system call that is required for acquiring and releasing the lock. \newline
One solution to work around the system call is to use Spinlocks, but Spinlocks are considered to be a bad idea in user space for the most part \cite{torvalds:spinlock}. Chapter 3 goes into more detail about this, when discussing busy-waiting in Redis’ I/O threading implementation. \newline
Fortunately, Linux offers a compromise: The futex system call makes it possible to develop
synchronization primitives around an atomic variable in user space, so that not every operation upon the synchronization primitive requires a system call. \newline
Mutexes can be implemented by utilizing the futex system call. When there is no contention, a mutex implementation with futex only requires a “cheap” atomic CAS operation in user space to acquire and release the lock, similar to a CAS Spinlock in user space. When there is contention and acquiring the lock at first try\footnote{This could also be implemented as a Spinlock-hybrid with multiple tries.} fails, the implementation falls back to the futex wait system call\footnote{Actually there is one multiplexed futex system call and the operation is just a parameter.}, which takes the memory address of the atomic variable and the expected value as an argument and executes a “compare-and-block” operation in kernel space. This operation is required to prevent a thread from being blocked indefinitely. This would otherwise be possible because the lock could already be released concurrently in the “time frame” from the CAS operation to the system call. The “compare-and-block” operation makes this safe and the futex system call immediately returns to user space in this case. On the other hand, if and only if the atomic compare operation in kernel space succeeds, the thread is blocked \cite{man:futex}.\newline
This Mutex can be unlocked by atomically storing a specific value in the (atomic) variable, which is a comparatively efficient operation, as long as there are no (blocked) waiters. If there are (blocked) waiters, the futex wake system call has to be invoked, which wakes up a specified amount of blocked waiters.\newline
This is the basic principle behind implementing Mutexes utilizing the futex system call from a high level perspective. Obviously, there is more nuance to real implementations, so I implemented the “mutex2” and “mutex3” algorithm that Ulrich Drepper introduces in the paper “Futexes Are Tricky” \cite{drepper:futex} in modern C++ with C++ memory barriers and included the source code in the appendix.\newline
Nowadays, the implementations behind popular “general purpose” Mutex interfaces, such as the POSIX “pthread\_mutex” or the “std::mutex” from the C++ standard template library, usually make use of the futex system call on Linux.
