\chapter{Performance Evaluations - Methodology}
\section{Approach}
\subsection{Overview}
While chapters 2 and 3 have a rather qualitative take on parallelism in highly concurrent network-bound applications, this and the following chapter’s main focus is on gathering and evaluating quantitative data by using experiments to distinguish key characteristics of some of the implementations that are described in the chapter about architectures \& paradigms. \newline
The goal is to verify some of the research findings presented in chapter 3 by performing experiments and examining the results. In the next section the essence of the research in regards to the selected applications is reviewed. Quantitative benchmark experiments are the tool of choice for testing the potential real world performance of a system. \newline
First of all, the benchmarks in this thesis are supposed to demonstrate that even the throughput of so-called network-bound applications can be bottlenecked by a single thread of execution. Secondly, the benchmark results should reflect the differences in how various highly concurrent network-bound applications incorporate parallelism into their implementation. It is important to note that benchmarks are always biased to a certain degree because they test a finite amount of scenarios. \newline
Further subsections discuss the selection of the candidates, the quantitative metrics, which build the basis for the performance evaluation, and the extensive pre-testing.

\subsection{Candidates}
In this study the selected application type of choice is in-memory databases. Common belief among developers is that in-memory databases are typically prone to be limited by the network bandwidth and therefore parallelism within the main logic is not necessary. In-memory databases do not require large amounts of CPU intensive calculations and are not limited by the performance of other resources such as the speed of the secondary storage, which might affect the performance of other highly concurrent network-bound applications like web servers. \newline
The candidates of choice are the in-memory databases Redis \cite{soft:redis}, KeyDB \cite{soft:keydb}, Mini-Redis \cite{soft:miniredis} and Redis Cluster.  \newline
Redis is among the candidates because it is the most popular open source in-memory database and it just recently introduced (optional) parallelism into the main logic. Before that there was a controversy in the developer communities about the necessity of parallelism for Redis. Redis without I/O threading is a plain single-threaded reactor. With I/O threading enabled, Redis’ main logic could be described as a synchronous producer-consumer reactor: On each event loop tick, read and write operations are distributed to consumer threads and the main thread synchronously waits for them to finish. This approach is supposed to perform the worst due to the overhead associated with initializing the I/O threading, the expensive task distribution and the inefficient busy-waiting of the I/O threads. \newline
KeyDB, the open-source multithreaded Redis fork, represents the multithreaded event loop model with a dedicated “epoll” instance per worker thread and static client assignment. This approach should produce the best throughput results among the shared-something approaches due to the multithreaded event loop and static client assignment, which has almost no runtime overhead. \newline
Mini-Redis makes use of Tokio and therefore represents an in-memory database with work-stealing scheduling within a lightweight asynchronous runtime that is driven by a single “epoll” instance. Since Mini-Redis is rather a “showcase” application that is developed by the team behind Tokio, Mini-Redis is the “wild card” among the candidates. However, according to the research, Tokio’s work stealing scheduler and task budgets should yield the best results in tail latencies. \newline
Redis Cluster represents the strict shared-nothing approach. There is no state shared between the nodes in the cluster during runtime. Even the state of the hash slots is stored on the clients. For the purpose of simplification, in this context thread is equal to node and a node is an operating system process that runs a single-threaded Redis server instance. To make a fair comparison, a Redis Cluster with N nodes is always compared to one of the other applications with N threads, if not stated otherwise. \newline
In uniform workload scenarios Redis Cluster should provide the best throughput results. There are no shared resources, which means that there are no locks on the core hash tables, client structures or elsewhere. There is no contention on socket or “epoll” structures within the kernel and no explicit coherence traffic because there is no shared memory.
Minimizing differences, which are not of interest for this thesis, is an important requirement, so all of the selected candidates have to support the same “Redis Serialization Protocol” (RESP) and this is the reason why the second most popular in-memory database “memcached” is not included in the tests. A single-threaded Redis instance serves as the point of reference for the following benchmarks.

\subsection{Performance Metrics}
As already hinted in the previous subsection, the performance metrics for the experiments in this study are throughput and response time percentiles. \newline
Throughput is usually measured in operations per second (OPS).  In a highly concurrent network-bound application an operation is usually a query or a request. Other more specific terms to describe OPS are queries per second (QPS) and requests per second (RPS). They are often used interchangeably. \newline
However, I decided to measure in bytes per second (B/s) for the following reasons: 
In the upcoming benchmark all applications have to process the same queries with the same payloads and they use the same protocol (RESP), which implies the same protocol overhead. For this artificial benchmark, specific QPS numbers are not of interest anyway. Instead, it is more interesting to demonstrate how close the candidates can get to the hard limit of the network, which is defined by the NIC and the network bandwidth. Measuring in bytes per second makes it easier to compare the candidates in regards to this. \newline
High response times percentiles are the performance metric to look out for in large distributed fan-out architectures. They directly affect the user-experience of a service. In sequential execution response times add up and even in parallel execution the slowest request sets the hard limit for the response time of the main request. Response times are measured in percentiles and high percentiles are known as tail latencies. Common percentiles are the P95, P99 and P99.9 percentile. They represent thresholds at which 95\%, 99\% and 99.9\% of requests are faster than that particular threshold. Amazon describes response time requirements for internal services in terms of the P99.9 percentile \cite[15]{kleppmann:data}. In-memory databases are usually leaf nodes in such distributed fan-out architectures and services aggregate multiple queries from such databases for one request. So this study even measures up to the P99.99 percentile.

\subsection{Pre-Testing}
Before performing the real benchmarks I wanted to take a closer look at Redis to explore how much computation is required proportionally for internal operations. I developed a small library for monitoring the Redis event loop. This library enables saving event loop ticks and measured time frames within an “event loop monitor” at runtime. Each monitor saves its data inside of a vector in memory during runtime. These monitors are inserted at certain points of interest, effectively creating a modified version of Redis. For example, these points of interest include pending writes to a socket, reading a query from a socket or the execution of commands. When the redis-server is stopped, the content of these monitors is saved to CSV files.\footnote{I did not use this version but instead an unmodified version of Redis for the “memtier” benchmarks.} \newline
These monitors suggest that Redis spends about 80\% of time within I/O operations during an event loop tick under a high load of SET and GET commands as long as the payloads are in the smaller than 10 kilobyte range. \newline
Roughly 7\% \footnote{~6\% for GET requests, ~8\% for SET requests} of time within an event loop tick is spent on the execution of the commands.\footnote{This was tested on the final benchmark setup.}
Write operations with payloads of 100kb or more are substantially more expensive in Redis. The histogram in figure \ref{fig:monitor_writes} displays the time that is required for executing the "writeToClient" function with varying payloads in Redis. This information is interesting for the final benchmarks because “throwing in” requests with larger payloads into the request mix likely contributes to skewing the task granularity.\newline
In KeyDB the access to the core hash table is guarded by a global lock. It is a custom implementation in assembler in the form of a hybrid Spinlock named “fastlock”, that calls “futex wait” after spinning for a lot of iterations and that is optimized for the common case of short lock intertervals. Mini-Redis does also lock the access to the core hash table, but Mini-Redis uses a “std::sync::Mutex”, which is implemented with futex on Linux.\footnote{Interestingly, the core hash table in Mini-Redis is a “std::collections::HashMap”, which is the Rust port of the “absl::flat\_hash\_map” developed by Google that is discussed in the section about SIMD in chapter 3.} \newline
The measurements can be utilized to approximate \textit{p}, which is the proportion of execution time that the part benefiting from multithreading originally occupied. Therefore, the measurements can give some approximate information about the potential speedup that can be expected by the different multithreaded approaches in Redis and KeyDB according to Amdahl’s law (see fig. \ref{fig:amdahl}). This helps for reasoning about the gathered data later on.
\footnote{While KeyDB and Redis share major code parts, there are still enough differences, that this approximate speedup cannot serve as a definitive hard limit.} If the results are very different from the described potential speedup, there is probably something wrong in the test scenario. 
\begin{figure}
    \centering
    \scalebox{0.5}{\input{./figs/monitor/clients_64_cmd_GET_fn_pending_writes.pgf}}
    \caption{Histogram of time required for executing "writeToClient" function with varying payloads in Redis.}
    \label{fig:monitor_writes}
\end{figure}
\begin{figure}
    \centering
    \scalebox{0.5}{\input{./figs/amdahl/amdahl.pgf}}
    \caption{Approximate speedup of Redis with I/O threading (p=0.8), KeyDB (p=0.93) and Redis Cluster (p=1) according to Amdahl's law.}
    \label{fig:amdahl}
\end{figure}
%\cleardoublepage
\section{Methods}
\subsection{Overview}
This section describes the methods used for gathering the benchmark results. \newline
It deals with the hardware and infrastructure selection, the implementation of the scripts, which are used for automating the execution of the benchmarks, the selection and the configuration of the benchmarks, as well as the configurations used for the candidates.
\subsection{Hardware and Infrastructure}
Cloud computing providers are the leading force in IT infrastructure for distributed computing. Amazon Web Services (AWS) was chosen as the provider for the benchmarks as it is the most popular cloud provider. \newline
Two AWS “m5zn.6xlarge” general purpose EC2 instances were selected each with 24 vCPU cores\footnote{2nd Generation Intel Xeon Scalable Processor Cascade Lake}, 96 GB RAM and full 50 Gbps network bandwidth that had Ubuntu 20.04 LTS (Linux Kernel 5.4) pre-installed. The benchmarks were run over a real network within the same AWS Virtual Private Cloud (VPC) and the same AWS Availability Zone (us-east-2b). \newline
The instance type “m5zn.6xlarge” was selected because it helps simulate a realistic scenario with its strong single core performance and relatively high network bandwidth.\footnote{A potential CPU bottleneck should not be provoked artificially, e.g by testing on a “free tier” EC2 instance.} It has \textit{full} access to 50 Gbps bandwidth, utilizing the Elastic Network Adapter (ENA). That is important because EC2 instance types with “up to ... Gbps” network bandwidth burst performance in certain scenarios, which could lead to distorted benchmarking results. Also headroom in bandwidth comes in handy for demonstrating CPU bottlenecks. The benchmarks should not get too close to fully saturating this bandwidth. The maximum memory bandwidth of the selected instance type is about 70GB/s according to the STREAMS 5.1 memory benchmark.

\subsection{Scripts}
I developed shell scripts that run the benchmarks on all candidates. Amongst other things these scripts aim to configure environments that are as similar as possible for all candidates. So I set each application up to deliver its best performance, at least to my knowledge. The shell scripts are available in the github repository\footnote{URL: https://github.com/lc0305/bachelor\_thesis} for this thesis.
The “main script” runs a “benchmark script” for each application. Redis, KeyDB and Mini-Redis share one benchmark script. For Redis Cluster I created a new script because it needed multiple adjustments. \newline
The “benchmark script” for Redis, KeyDB and Mini-Redis performs the memtier benchmark on each thread (1, 2, 4, 8, 12) and client (24, 96, 192, 384, 768) configuration. These configurations were selected because the EC2 instances that were used for the tests had 24 vCPU cores. Using all available cores for the benchmark makes it unlikely that it becomes the bottleneck. So the benchmark was allowed to make use of all the available 24 cores for each run, while the database server was only allowed to use 12 cores, which corresponds to a maximum configuration of 12 threads for the databases. Running the benchmark with a fixed amount of 24 threads meant that the lowest client configuration was 24 clients (one client per thread). The other client configurations were multiples of 24 and went up to 768 clients because I assumed that in-memory databases with more than 768 concurrent clients were unlikely. In the following sections the 24 client configuration is referred to as \textit{low concurrency}, the 192 client configuration as \textit{medium concurrency} and the 768 client configuration as \textit{high concurrency}. \newline
The script iterates over these configurations. With each thread configuration the script starts the properly configured database server on the other target server instance via remote SSH. After each startup, the database serves 2M requests for “warm up” without performance measurements enabled. When the warmup finishes, the script performs the actual “memtier benchmark” runs with 10M requests each on the remote database server for each client configuration. When all runs for a thread configuration are finished, the script “kills” the remote database server. The database server restarts with a new thread configuration in the next iteration. \newline
Redis Cluster requires at least three nodes, so that is why the thread/node configurations in the cluster script start at four nodes. Other noteworthy changes in the cluster script are that it sets up and destroys a whole cluster for each mentioned iteration on the remote server via SSH.

\subsection{Memtier benchmark configuration}
“Memtier” is a popular benchmarking tool for performance testing NoSQL key-value databases \cite{soft:memtier}. The “memtier benchmark” can spawn multiple threads and each thread independently generates requests for one or more “simulated” client connections. When responses for the generated requests are received, the benchmark sends new requests according to the configuration. This process repeats in a loop and breaks when a certain limit is reached. This limit could be a discrete request count or a “real” time frame. The benchmark records the throughput (in operations per second and kilobytes per second) and the response time percentiles for the requests. Results are saved to files persistently when the benchmark is finished. The “memtier benchmark” is being developed by the “Redis Labs Team”. \newline
In this study, the “memtier benchmark” is configured to send SET and GET queries with a 1:2 ratio and uniform random distribution. 99\% of the SET and GET values are configured to have a data size of 100 bytes. The 100-byte requests represent requests with a small payload. Based on the pre-testing, varying payloads below the one kilobyte threshold does not make a big difference. The other 1\% of SET and GET values in this configuration have a data size of 100 kilobytes and represent a larger payload. \newline
The pre-testing demonstrated that write operations to the clients are substantially more expensive with the benchmark configured to data sizes of 100kb compared to data sizes below 1kb. Larger data sizes might also lead to Redis and KeyDB installing the write handler: \newline
Redis adds clients with pending writes to the pending writes queue and processes the clients in this write queue before calling “epoll\_wait”. The write handler is only installed when the non-blocking write returns with “EAGAIN” \textbf{or} when the server has sent more than 64kb of data to a client \textbf{and} the client has more pending data in the reply buffer within one event loop tick.\footnote{A socket is almost always ready to be written to, so when registering write interest with level-triggered notifications, like Redis does, “epoll\_wait” always returns immediately. The solution would be to constantly deregister the write interest again with “epoll\_ctl” after finishing a reply. However, this was probably ruled out as being too inefficient by the developers.} \newline A small amount of more demanding requests does not immediately saturate the NIC, but does contribute to skewing the overall task granularity, which is desirable for a benchmark that is also testing tail latency performance. Unfortunately, my tests indicate that these “large” requests are grouped together.  \newline Furthermore, it is not possible to configure non-uniform client load in “memtier” to my best knowledge. However, developing an improved benchmark for NoSQL key-value databases is beyond the scope of this thesis. \newline
In this study “memtier” is configured to write the main data, including the average throughput, to a JSON file and response time percentiles for High Dynamic Range (HDR) Histograms to one more separate file.

\subsection{Redis (Cluster) and KeyDB configuration}
Redis and KeyDB have persistence with weak durability enabled by default. This has usually low impact on performance, but can make the Redis process “fork” for log rewriting of the append-only fashion (AOF) files. Since I did not want any contending processes during my benchmark, I disabled AOF and RDB persistence completely by setting the “--save ‘ ‘“ and “--appendonly no” arguments.  \newline
By default, Redis I/O threading only fans out the write operations to the consumer threads. However, I wanted to measure the full multi-threaded potential of Redis, so I enabled threaded read operations with “--io-threads-do-reads yes”.  \newline
I did not enable Redis I/O threading for the nodes in Redis Cluster.  \newline
Furthermore, I added the “--min-clients-per-thread 1” flag to KeyDB to get rid of the threshold that is described in chapter 3, which KeyDB uses by default for distributing connections to the worker threads.

\subsection{Mini-Redis configuration}
Some minor adjustments are required to make the Mini-Redis Server work with the benchmark tests in this study. Because it is a “showcase application” the “maximum connections" parameter is not configurable, but instead hard coded, so I manually set it to 10k in the code. Mini-Redis had to be recompiled for every thread configuration because the thread count is handled by a Tokio Macro at Mini-Redis’ current stage and is not configurable during runtime. In this study, the Mini-Redis server is not linked against the default allocator, but instead against “jemalloc”, because both Redis and KeyDB build with “jemalloc” on Linux by default. In-memory databases make frequent use of the allocator and I tried to reduce the influences of differences between the candidates that are not of interest for this thesis.

\section{Analysis}
\subsection{Overview}
This section describes the approach to analysing the data. All the relevant quantitative evaluations are done within “Jupyter-Notebooks”. The notebooks are available in the github repository\footnote{URL: https://github.com/lc0305/bachelor\_thesis} for this thesis.  \newline
The goal of the analysis is to show how high concurrency network-bound applications \textit{can} benefit from multi-core CPUs and capture obvious trends in the gathered data.
For more comprehensive results, further benchmarks with varying hardware and software configurations would have to be done. So the upcoming results should not be misinterpreted as definitive assessments for the tested candidates.
\subsection{Throughput}
For the evaluation of the throughput it is of interest how much average (mean) throughput (in MB/s) the different candidates produce in regards to varying levels of concurrency and parallelism. The level of concurrency is measured in concurrent clients. The level of parallelism is measured in the amount of threads or processes that the candidate makes use of. In a two-dimensional plot one of these two variables has to be constant, so each combination is plotted twice. For simplification only the 24 (low concurrency), 192 (medium concurrency) and 768 (high concurrency) configurations are selected as constants for the plots if the thread count is the variable. \newline
Heatmaps were used to make substantial differences in throughput results of each client-thread configuration compared to a single-threaded Redis instance visible. The single single-threaded Redis instance serves as the point of reference for all benchmarks.
\subsection{Response times}
A High Dynamic Range (HDR) Histogram is plotted based upon the response time percentile data generated by “memtier” for each client-thread configuration until the P99.99 percentile. Thus the slowest 1k requests of the 10M requests for each plotted combination are discarded. HDR Histograms make differences in tail latency performance visible.
Multiple heatmaps capture substantial differences between the selected candidates compared to the point of reference, using the measured P95, P99, P99.9 and P99.99 response time percentiles.

\chapter{Performance Evaluations - Results}

\section{Throughput}

\subsection{Plots}
\input{03_throughput}
\pagebreak

\subsection{Key Observations}
\begin{figure}
    \centering
    \scalebox{0.5}{\input{./figs/average_throughput/average_throughput.pgf}}
    \caption{Average (mean) throughput of Redis, KeyDB, Mini-Redis and Redis Cluster with varying thread counts.}
    \label{fig:avg_throughput}
\end{figure}
The shared-something approaches (Redis, KeyDB, Mini-Redis) scale poorly across multiple threads with 24 clients (low concurrency). Redis’ I/O threading throughput even degraded slightly with 8 threads compared to the point of reference in this configuration. This can be observed best in the heatmaps in table \ref{tbl:table_of_figures_throughput_tables}. The Redis Cluster on the other hand scaled much better with low concurrency. This can probably be attributed to the fact that each client in cluster-mode establishes a TCP connection with each node in the cluster. So depending on the thread/node configuration, the amount of global concurrent connections on the server is:\newline
\centerline{$connections_{total}(nodes) = clients_{configured} * nodes$}
In this study, Redis Cluster establishes four to twelve times more concurrent connections in total than the shared-something approaches. \newline
When increasing the amount of concurrent clients, the shared-something approaches start reaching their potential. At around 192-384 clients the speedup from adding clients plummets. The throughput of the shared-nothing Redis Cluster degrades in all thread configurations at 768 concurrent clients. The graphs in the tables \ref{tbl:table_of_figures_throughput_t1} and \ref{tbl:table_of_figures_throughput_t2} shows this very well. This is most likely also related to the higher amount of total connections in Redis Cluster. With 12 nodes the cluster has to serve up to 9216 concurrent connections. \newline
Up to 4 configured threads and under the condition that there are enough concurrent clients, KeyDB lives up to its “marketing hype” in this study: Compared to a single-threaded Redis instance (point of reference) KeyDB delivers a 79-88\% increase in throughput with equal to or more than 96 clients and 2 worker threads and it still delivers a respectable 209-224\% increase in throughput with 192 or more clients and 4 worker threads. However, adding more than 4 threads to KeyDB seems to be a waste of resources for the purpose of improving the throughput, because there are no substantial further improvements (See fig. \ref{fig:avg_throughput}) and performance even degrades in some scenarios. At this point, it has to be mentioned that KeyDB’s developer does not recommend assigning more than 4 worker threads to KeyDB \cite{soft:keydb}. My educated guess is that this might be related to the complex locking strategies within KeyDB and the custom implementation of KeyDB’s “fastlock”. Further investigation is required for verification.\footnote{One possible method that comes to my mind is exchanging KeyDB’s “fastlock” with a general purpose “std::mutex” mutex lock implementation.} \newline
Adding threads to Mini-Redis increases the throughput monotonically. Interestingly, when there are more than 24 concurrent clients, the plot of Mini-Redis’ throughput over threads seems to be much closer to the predicted “Amdahl speedup” for KeyDB than KeyDB itself. This might be attributed to Mini-Redis using a general purpose Mutex implementation for the core hash table or a possibly more efficient thread utilization in Tokio or a combination of both. Again further investigation is required to draw final conclusions. So although Mini-Redis has a “slower start” than KeyDB\footnote{This is probably because the single-threaded Mini-Redis also delivers less throughput than Redis and KeyDB.} Mini-Redis catches up at 8 threads and surpasses KeyDB in throughput at about 12 threads. \newline
When looking at throughput, Redis’ approach to multithreading scales particularly poorly in this test. Redis’ I/O threading with 12 threads improves the performance compared to the point of reference, which is the single-threaded Redis instance, by about 80\% on average (mean). In comparison, a KeyDB server instance with just 2 threads achieves almost the same throughput on average. The results of Redis’ I/O threading in this study seem to be about in line with the results of an engineer at “RedisLabs” \cite{redis:io_thread}.\newline
Redis Cluster scales almost linearly with 4 nodes (threads) in certain scenarios. For example, a 4-node cluster with 192 configured clients delivers 267\% more throughput than one single-threaded Redis instance. The corresponding heatmap in table \ref{tbl:table_of_figures_throughput_tables} displays this well. When more nodes are added to the cluster, the throughput still improves, but the speedup is far off from the linear mark. Still Redis Cluster delivers the highest throughput and leads with substantial margins among the tested candidates for all configurations with more than 4 threads. This can be observed best by looking at the average (mean) throughput results in the plot in figure \ref{fig:avg_throughput} and the heatmap in figure \ref{fig:avg_throughput_heat}. 
\begin{figure}
    \centering
    \scalebox{0.6}{\input{./figs/average_throughput/heatmap.pgf}}
    \caption{Average (mean) throughput compared to reference point (single-threaded Redis) in \% (higher is better).}
    \label{fig:avg_throughput_heat}
\end{figure}

\subsection{Assumptions in Retrospect}
All approaches deliver substantial speedups compared to a single-threaded Redis instance. In most tested scenarios a single-threaded Redis instance (point of reference) does not even get close to fully saturating the capabilities of the system. A Redis Cluster with 12 nodes (threads) is able to increase throughput by up to 667\% compared to a single-threaded Redis instance. This proves that single-threaded high concurrency network-bound applications are not inherently hard limited by the network bandwidth.\newline
KeyDB was expected to provide the highest average throughput among the shared-something approaches and up to a thread count of 4 it did. Although KeyDB’s developer does not recommend using KeyDB with more than 4 threads, scaling that poorly beyond 4 threads is a surprise. \newline
Mini-Redis’ performance in throughput is in-between KeyDB and Redis’ I/O threading in most tested scenarios. This corresponds to the assumptions made in chapter 3, too. However, unexpectedly Mini-Redis’ average throughput surpassed KeyDB with 8 or more threads. This can mainly be attributed to KeyDB’s poor scaling when running on more than 4 threads. \newline
The shared-nothing Redis Cluster, as expected, delivered the best performance in regards to total throughput. In every single test case Redis Cluster provided the highest numbers for throughput. However, for higher node (thread) counts even Redis’ Cluster is far off from scaling linearly in the tested benchmarks. \newline
Redis’ I/O threading on the other hand delivered the worst throughput results, which was also expected.

\section{Response Time Percentiles}

\subsection{Plots}
\input{03_latencies}
\pagebreak

\subsection{Key Observations}
\begin{figure}
    \centering
    \scalebox{0.6}{\input{./figs/average_latencies/latencies_percentile_avg.pgf}}
    \caption{Average (mean) response time percentiles compared to reference point (single-threaded Redis) in \% (lower is better).}
    \label{fig:avg_latency_heat}
\end{figure}
All the shared-something candidates have substantially improved P95, P99, P99.9 and P99.99 response time percentiles on average (mean) compared to the single-threaded Redis server instance (point of reference). The heatmap in figure \ref{fig:avg_latency_heat}, which displays the average (mean) response time percentiles compared to the point of reference, gives a good overview of this. This demonstrates that the improvements that multi-core scaling provides for highly concurrent network-bound applications is not only limited to throughput. 
The “sweet spot” for response time percentile improvements compared to the point of reference seems to be around 192-384 concurrent clients in this study. All tested applications show higher response time percentiles when increasing the amount of concurrent connections in the benchmark. That is why the HDR histograms use different scales with \textit{Iow concurrency} (24 clients), \textit{medium concurrency} (192 clients) and \textit{high concurrency} (768) clients in the corresponding figures (see table \ref{tbl:table_of_figures_t1}, table \ref{tbl:table_of_figures_t2}, table \ref{tbl:table_of_figures_t4}, table \ref{tbl:table_of_figures_t8} and table \ref{tbl:table_of_figures_t12}).  \newline
On the other hand, increasing the thread count generally improves tail latencies slightly in all applications except for the shared-nothing Redis Cluster, where increasing the amount of nodes leads to deteriorating tail latency results in most cases. \newline 
Interestingly, Mini-Redis seems to struggle when there are equal to or more than 192 concurrent connections per thread.\footnote{In Tokio’s work stealing scheduler implementation, worker threads have not a statically assigned fixed set of connections like in KeyDB.} This can be observed particularly well in the configuration with 768 clients in table \ref{tbl:table_of_figures_t2} or alternatively in the heatmaps of the P95 and P99 percentile in the configurations with 384 and 768 clients in table \ref{tbl:table_of_figures_95_response_times} and \ref{tbl:table_of_figures_99_response_times}. My personal hypothesis is that this might be related to the capacity of the run queue, which stores up to 256 Tasks.\footnote{If this local run queue is full, tasks are pushed onto a global task queue.} However, when there are less than 192 connections per thread, Mini-Redis delivers substantially lower tail latencies than the other applications. In fact, when Mini-Redis’ “outliers” are removed it beats the other applications in regards to P99.9 and P99.99 response time percentiles in every single case, sometimes even by a substantial margin. On average (mean), the differences of Mini-Redis’ P95 and P99 response time percentiles to the point of reference are almost equivalent to corresponding results of KeyDB, once these “outliers” are removed. The heatmap in figure \ref{fig:avg_latency_heat_out} displays the average (mean) response time percentiles without these “outliers” compared to the point of reference. \newline 
\begin{figure}
    \centering
    \scalebox{0.6}{\input{./figs/average_latencies/latencies_percentile_avg_woutliers.pgf}}
    \caption{Average (mean) response time percentiles without Mini-Redis' outliers compared to reference point in \% (lower is better).}
    \label{fig:avg_latency_heat_out}
\end{figure}
Redis’ I/O threading offers the least improvements in tail latencies among the shared-something applications in most cases, which is analogous to the results in throughput. However, in a lot of cases tail latencies in Redis’ I/O threading are not that different from KeyDB’s and Mini-Redis’ results. Redis’ I/O threading even beats Mini-Redis in regards to the average (mean) P95 and P99 percentiles, but this is mainly due to Mini-Redis delivering untypical results when there are more than or equal to 192 connections per thread. \newline 
KeyDB’s improvements in tail latencies compared to the point of reference are consistently better than the improvements of Redis’ I/O threading. KeyDB has lower P95 and P99 percentiles than Mini-Redis (see table \ref{tbl:table_of_figures_95_response_times} and \ref{tbl:table_of_figures_99_response_times}), but Mini-Redis performs better in the P99.9 and P99.99 percentiles (see table \ref{tbl:table_of_figures_999_response_times} and \ref{tbl:table_of_figures_9999_response_times}). \newline 
Overall Redis Cluster brought up the rear in tail latency performance. This can probably be attributed to an observation made earlier: When benchmarking Redis Cluster, each client establishes a TCP connection with each node in the cluster. Increasing the amount of concurrent connections worsens the tail latency results for all applications.

\subsection{Assumptions in Retrospect}
The major assumption was that Tokio’s work-stealing scheduling should give Mini-Redis an advantage in regards to tail latency performance. Mini-Redis generally performed well, but in scenarios where the amount of clients per thread was greater or equal to 192, Mini-Redis performed particularly poorly and delivered much worse tail latency results than all the other tested applications. \newline
Furthermore, the tested workloads in the “memtier” benchmark are not particularly skewed, because as explained earlier, the 1\% bigger, more expensive queries are unfortunately grouped together. This most likely leads to all worker threads processing demanding queries at about the “same time" and less “stealing” in Mini-Redis’ work stealing scheduler. A “truly skewed” workload would probably have been better for showcasing the full benefits that work stealing scheduling can provide. So the fact that Mini-Redis (Tokio) delivered the best results on the P99.9 and P99.99 response time percentiles might rather be attributed to Tokio limiting the task budgets.\newline
What is also interesting is that the shared-nothing database Redis Cluster delivered the worst tail latency results in the conducted tests. This opposes the results from the paper “The Impact of Thread-Per-Core”, which was discussed in chapter 3 in the section about “thread-per-core”. However, the database described in this paper is also not as “strict shared-nothing" as Redis Cluster.
\section{Assessment and Further Discussion}
The shared-nothing database Redis Cluster offers by far the best improvements in regards to throughput in the benchmark tests. However, a Redis Cluster needs to consist of at least three nodes and its tail latency results, on the other hand, are the worst with a substantial margin. It is the only candidate that almost always delivered even worse tail latency results than a single-threaded Redis instance under full load. \newline 
KeyDB lives up to its marketing claims when configured with up to 4 threads and offers substantial improvements in throughput and tail latencies when compared to a single threaded Redis instance. \newline 
Redis’ I/O threading is the shared-something approach which performs the worst in both throughput and tail latencies. \newline 
The poor results of Redis’ I/O threading in both performance metrics in this study might be attributed to the overhead associated with Redis’ I/O threading. Redis’ I/O threading has to be activated and can be stopped, although that is unlikely in a high load scenario. When it is activated, read and write operations are distributed to the consumer threads on each event loop tick and the main thread synchronously waits for the consumers to finish. There is neither a similar overhead in KeyDB nor in Mini-Redis (Tokio): \newline KeyDB assigns connections statically and each thread handles its own part of connections. The Tokio Tasks in Mini-Redis are only stolen from another thread, when a worker has no other task to process. The overhead in Redis’ I/O threading is described in more detail in chapter 3 about architectures \& paradigms. \newline 
Overall, in low to medium concurrency scenarios, Mini-Redis (Tokio) performs the best in regards to tail latencies. This would most likely be the scenario for something like an in-memory database. Mini-Redis performs well enough in high concurrency situations if it has enough worker threads for handling the connections. The implications of extreme concurrency situations, such as HTTP based server applications with thousands of concurrent connections, on Tokio’s performance will have to be investigated more thoroughly. \newline 
Furthermore, Redis and KeyDB both support command pipelining. However, all these tests were performed with command pipelining disabled. Command pipelining is supposed to trade latency for throughput. Whether this command pipelining could improve Redis’ or KeyDB’s operations per second to a level similar to Redis Cluster, while aiming for similar latency results on a single (physical) node, would be an interesting starting point for further research.


%\input{03_avg_throughput}
%\input{03_throughput}
%\input{03_avg_latency}
%\input{03_avg_latency_outlier}
%\input{03_throughput}